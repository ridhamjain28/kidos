# IBLM Core - The Breakthrough Context Engine

## ğŸ§  What is IBLM?

**IBLM** (Individual Behavior Learning Machine) is a revolutionary context management system that learns **WHO a user is**, not just what they've said. It acts as a "digital brain" between the user and any LLM.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              IBLM ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   USER â”€â”€â”€â”€â”€â”€â–º INTERACTION â”€â”€â”€â”€â”€â”€â–º IBLM â”€â”€â”€â”€â”€â”€â–º LLM                        â”‚
â”‚                OBSERVER          KERNEL       (Any Model)                   â”‚
â”‚                    â”‚                â”‚                                       â”‚
â”‚                    â–¼                â–¼                                       â”‚
â”‚              Extract Signals â†’ Compile Rules â†’ Inject Context              â”‚
â”‚                    â”‚                â”‚                                       â”‚
â”‚                    â–¼                â–¼                                       â”‚
â”‚              [Corrections]    [Living Memory]                               â”‚
â”‚              [Preferences]    [User Profile]                                â”‚
â”‚              [Expertise]      [Style Vector]                                â”‚
â”‚              [Entities]       [Knowledge Graph]                             â”‚
â”‚                                                                             â”‚
â”‚                         GARBAGE COLLECTION                                  â”‚
â”‚                    Raw logs deleted after compilation                       â”‚
â”‚                         â†“                                                   â”‚
â”‚                    INFINITE SCALING                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ The Breakthrough: Why IBLM is Different

### Traditional RAG (Retrieval Augmented Generation)

```
User: "Use Python"              â†’ Store: "Use Python"
AI: "Here's Python code..."     â†’ Store: "Here's Python code..."
User: "No, use TypeScript"      â†’ Store: "No, use TypeScript"  
AI: "Converting to TS..."       â†’ Store: "Converting to TS..."
...1000 more messages...

Result: 100,000+ tokens stored
Problem: Token bloat, context window limits, slow retrieval
```

### IBLM (Recursive Context Compiler)

```
User: "Use Python"              â†’ Signal: PREFERENCE(Python)
AI: "Here's Python code..."     â†’ 
User: "No, use TypeScript"      â†’ Signal: CORRECTION(TypeScript > Python)
AI: "Converting to TS..."       â†’

COMPILATION:
  Rule: "User prefers TypeScript over Python" (weight: 0.9)
  
GARBAGE COLLECTION:
  Source messages: DELETED
  
Result: ~20 tokens stored (the rule)
Scaling: O(log n) instead of O(n)
```

---

## ğŸ“Š Token Efficiency Comparison

| Interactions | Traditional RAG | IBLM Core |
|--------------|-----------------|-----------|
| 10           | ~5,000 tokens   | ~100 tokens |
| 100          | ~50,000 tokens  | ~300 tokens |
| 1,000        | ~500,000 tokens | ~500 tokens |
| 10,000       | ~5,000,000 tokens | ~800 tokens |

**IBLM grows logarithmically** because it *compiles* knowledge into rules.

---

## ğŸ”„ The IBLM Workflow

### Phase 1: Observation

```python
from iblm import IBLM

brain = IBLM()

# Every interaction is observed
brain.observe(
    user_input="I'm building a FastAPI backend",
    ai_output="Great! Here's how to structure it..."
)
```

**What happens internally:**
1. `InteractionObserver` analyzes the input
2. Extracts signals: `ENTITY(FastAPI)`, `CONTEXT(backend)`
3. Signals are queued for compilation

### Phase 2: Signal Extraction

The Observer detects **implicit signals** without requiring explicit user input:

| User Says | Signal Detected | Confidence |
|-----------|-----------------|------------|
| "No, use TypeScript instead" | CORRECTION | 0.95 |
| "I prefer concise responses" | PREFERENCE | 0.80 |
| "I'm an expert in ML" | EXPERTISE | 0.85 |
| "Working on Project Alpha" | ENTITY | 0.70 |
| Short messages | STYLE(concise) | 0.60 |

### Phase 3: Evolution (The Brain)

```python
# The evolve() algorithm:
def evolve(signals):
    # 1. Detect contradictions with existing rules
    for signal in signals:
        if contradicts(existing_rules):
            penalize_contradicted_rules()
    
    # 2. Cluster similar signals
    clusters = cluster_by_similarity(signals)
    
    # 3. Compile clusters into rules
    for cluster in clusters:
        if cluster.count >= threshold:
            rule = compile_to_rule(cluster)
            add_to_kernel(rule)
            delete_source_signals()  # GARBAGE COLLECTION
    
    # 4. Decay and prune low-weight rules
    for rule in kernel:
        rule.weight -= DECAY_RATE
        if rule.weight < MIN_WEIGHT:
            delete(rule)
    
    # 5. Consolidate similar rules
    merge_similar_rules()
```

### Phase 4: Context Injection

```python
# When user asks a new question
context = brain.inject("How do I implement auth?")

print(context.system_header)
# Output:
# USER CONTEXT: Software engineer, expert in Python and FastAPI.
# COMMUNICATION STYLE: Prefers concise, technical responses.
# PREFERENCES:
# - Use TypeScript over JavaScript
# - Include type hints
# - Prefer async/await patterns
# ACTIVE PROJECT: FastAPI Backend - API development
```

The LLM now has **"telepathic"** understanding of the user.

---

## ğŸ¯ Individual Behavior Learning

The "Individual" in IBLM means we build a complete picture of **WHO the user is**:

### UserProfile (The Digital Twin)

```python
profile = {
    "role": "Senior Software Engineer",
    "industry": "FinTech",
    "expertise_domains": ["Python", "FastAPI", "PostgreSQL"],
    "expertise_levels": {"Python": 0.9, "FastAPI": 0.8},
    "preferred_languages": ["Python", "TypeScript"],
    "avoided_technologies": ["PHP", "jQuery"],
    "traits": {
        "perfectionist": 0.7,
        "pragmatic": 0.4,
        "systematic": 0.8
    },
    "active_goals": ["Build scalable API", "Migrate to microservices"],
    "profile_confidence": 0.85  # Increases with more interactions
}
```

### StyleVector (How to Communicate)

```python
style = {
    "formality": 0.3,      # More casual
    "verbosity": 0.2,      # Prefers concise
    "technicality": 0.9,   # Very technical
    "directness": 0.8,     # Prefers blunt
    "pace": 0.7            # Fast-paced
}
```

This vector is used to calibrate **how** the AI responds.

---

## ğŸ”® The Reflection Prompt

IBLM can generate a prompt that makes the LLM **embody the user**:

```python
reflection = brain.get_reflection_prompt()

# Output:
# "You are embodying the perspective of the following user. 
#  Respond as if you ARE this person:
#
#  I am a Senior Software Engineer in the FinTech industry. 
#  I'm an expert in Python, FastAPI. I prefer working with 
#  Python, TypeScript. I avoid PHP. My communication style: 
#  casual and friendly; prefers concise responses; comfortable 
#  with technical depth. I'm currently focused on: Build 
#  scalable API."
```

Use this for:
- **Planning**: "What would the user want next?"
- **Review**: "Evaluate this from the user's perspective"
- **Continuation**: AI can continue work in user's style

---

## ğŸ’¾ Portability

The entire brain exports to a single file:

```python
# Save
brain.save("my_brain.json.gz")  # Compressed

# Load anywhere
brain = IBLM.load("my_brain.json.gz")

# Works with any LLM
# The context is model-agnostic!
```

**Brain size after 1000 interactions: ~50KB** (compressed)

---

## ğŸš€ Quick Start

```python
from iblm import IBLM

# Create a brain
brain = IBLM()

# Set project context
brain.set_project("MyApp", "React + FastAPI web application")

# Teach it directly
brain.teach("I'm an expert Python developer")
brain.teach("Always use type hints")
brain.teach("I prefer functional programming patterns")

# Observe interactions
brain.observe("Write me a login endpoint", "Here's the endpoint...")
brain.observe("Make it async", "Converting to async...")

# Get context for new prompt
injection = brain.inject("How do I add rate limiting?")
print(injection.system_header)

# Save for later
brain.save("my_brain.json.gz")
```

---

## ğŸ—ï¸ Architecture Deep Dive

### Component Responsibilities

| Component | Responsibility |
|-----------|----------------|
| `InteractionObserver` | Watches (user, AI) pairs, extracts signals |
| `LogicCompiler` | Converts signals â†’ rules, implements evolve() |
| `UserKernel` | Stores rules, nodes, profile, style vector |
| `ContextInjector` | Builds optimized system headers |
| `EmbeddingEngine` | Semantic matching without external APIs |
| `IBLM` | Unified API orchestrating all components |

### The evolve() Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     evolve() ALGORITHM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  INPUT: New signals from observer                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 1. CONTRADICTION DETECTION                              â”‚â”‚
â”‚  â”‚    For each CORRECTION signal:                          â”‚â”‚
â”‚  â”‚    - Find rules with high semantic similarity           â”‚â”‚
â”‚  â”‚    - If negation detected â†’ mark as contradicted        â”‚â”‚
â”‚  â”‚    - Apply weight penalty (-0.5)                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 2. SIGNAL CLUSTERING                                    â”‚â”‚
â”‚  â”‚    Group signals by:                                    â”‚â”‚
â”‚  â”‚    - Signal type (PREFERENCE, STYLE, etc.)              â”‚â”‚
â”‚  â”‚    - Semantic similarity (>0.75)                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 3. RULE COMPILATION                                     â”‚â”‚
â”‚  â”‚    For each cluster:                                    â”‚â”‚
â”‚  â”‚    - If count >= threshold AND confidence >= 0.5        â”‚â”‚
â”‚  â”‚    - Generate rule with computed weight                 â”‚â”‚
â”‚  â”‚    - Corrections get +0.3 weight boost                  â”‚â”‚
â”‚  â”‚    - Add to kernel                                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 4. PROFILE & STYLE UPDATES                              â”‚â”‚
â”‚  â”‚    - Update expertise domains                           â”‚â”‚
â”‚  â”‚    - Update language/tool preferences                   â”‚â”‚
â”‚  â”‚    - Adjust style vector dimensions                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 5. GARBAGE COLLECTION                                   â”‚â”‚
â”‚  â”‚    - Delete processed interaction logs                  â”‚â”‚
â”‚  â”‚    - Keep only content hashes (deduplication)           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 6. DECAY & PRUNING                                      â”‚â”‚
â”‚  â”‚    - Apply time-based decay to all rule weights         â”‚â”‚
â”‚  â”‚    - Remove rules with weight < 0.1                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 7. CONSOLIDATION                                        â”‚â”‚
â”‚  â”‚    - Find rules with similarity > 0.8                   â”‚â”‚
â”‚  â”‚    - Merge into single rule                             â”‚â”‚
â”‚  â”‚    - Combine weights                                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                              â”‚
â”‚  OUTPUT: EvolutionReport with statistics                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ†š IBLM vs Existing Solutions

| Feature | Mem0/etc | RAG Systems | IBLM Core |
|---------|----------|-------------|-----------|
| Storage | Raw text | Raw text + vectors | Compiled rules |
| Scaling | O(n) | O(n) | **O(log n)** |
| Learning | Passive | Passive | **Active** |
| User Model | None | None | **Complete profile** |
| Corrections | Manual | Manual | **Automatic** |
| Portability | Limited | Complex | **Single file** |
| Token Usage | High | High | **Minimal** |
| External APIs | Required | Required | **Optional** |

---

## ğŸ“ File Structure

```
IBLM/
â”œâ”€â”€ __init__.py      # Package exports
â”œâ”€â”€ models.py        # Data structures (Signal, Rule, Node, etc.)
â”œâ”€â”€ embeddings.py    # Lightweight embedding engine
â”œâ”€â”€ kernel.py        # UserKernel (living memory)
â”œâ”€â”€ observer.py      # InteractionObserver (signal extraction)
â”œâ”€â”€ compiler.py      # LogicCompiler (evolve algorithm)
â”œâ”€â”€ injector.py      # ContextInjector (prompt enhancement)
â”œâ”€â”€ iblm.py          # Main IBLM class (unified API)
â””â”€â”€ tests/
    â””â”€â”€ test_iblm.py # Comprehensive test suite
```

---

## ğŸ”’ Production Features (v2.0)

IBLM v2.0 introduces enterprise-ready features for security, reliability, and scalability.

### Input Validation & Sanitization

```python
from iblm import IBLM, IBLMConfig

# Enable validation (on by default)
config = IBLMConfig(enable_validation=True)
brain = IBLM(config)

# All inputs are automatically:
# - Sanitized for XSS attacks
# - Truncated to safe lengths
# - Checked for injection patterns
brain.observe(user_input, ai_output)  # Safe!
```

### Thread Safety

```python
# Thread-safe operations (on by default)
config = IBLMConfig(enable_thread_safety=True)
brain = IBLM(config)

# Safe for concurrent access
import threading
threads = [
    threading.Thread(target=brain.observe, args=(input, output))
    for input, output in messages
]
```

### Resource Limits (DoS Protection)

```python
# Prevent unbounded memory growth
config = IBLMConfig(
    max_rules=1000,      # Maximum behavioral rules
    max_nodes=500,       # Maximum knowledge nodes
    max_pending_signals=100,  # Signal buffer limit
)
brain = IBLM(config)
```

### Encrypted Persistence

```python
# Save with AES-256 equivalent encryption
brain.save_encrypted("brain.iblm.enc", password="secure_password_123")

# Load encrypted brain
brain = IBLM.load_encrypted("brain.iblm.enc", password="secure_password_123")

# Wrong password â†’ IntegrityError (tamper detection)
```

### Health Checks

```python
# Monitor brain health
health = brain.health_check()
print(health)
# {
#     "status": "healthy",
#     "id": "abc12345",
#     "rules_count": 42,
#     "nodes_count": 15,
#     "observations": 500,
#     "profile_confidence": 0.85,
#     "kernel_metrics": {...}
# }
```

### Context Manager

```python
# Automatic cleanup
with IBLM() as brain:
    brain.teach("I'm a Python developer")
    brain.observe(user_input, ai_output)
    # ...
# Brain automatically closes and optionally saves
```

### Production Configuration

```python
# Full production config
config = IBLMConfig(
    # Security
    enable_validation=True,
    enable_thread_safety=True,
    enable_rate_limiting=True,
    
    # Behavior
    auto_evolve=True,
    auto_gc=True,
    gc_threshold=20,
    
    # Limits
    max_rules=1000,
    max_nodes=500,
)
brain = IBLM(config)
```

### Custom Exceptions

```python
from iblm import (
    IBLMError,           # Base exception
    ValidationError,     # Input validation failed
    SecurityError,       # Security issue
    ResourceLimitError,  # Limit exceeded
    IntegrityError,      # Data corruption/tampering
    EncryptionError,     # Encryption/decryption failed
    RateLimitError,      # Rate limit exceeded
)

try:
    brain.observe(malicious_input, output)
except ValidationError as e:
    print(f"Input rejected: {e}")
```

---

## ğŸ“ File Structure (v2.0)

```
IBLM/
â”œâ”€â”€ __init__.py      # Package exports (v2.0)
â”œâ”€â”€ models.py        # Data structures (Signal, Rule, Node, etc.)
â”œâ”€â”€ embeddings.py    # Lightweight embedding engine
â”œâ”€â”€ kernel.py        # UserKernel (thread-safe living memory)
â”œâ”€â”€ observer.py      # InteractionObserver (signal extraction)
â”œâ”€â”€ compiler.py      # LogicCompiler (evolve algorithm)
â”œâ”€â”€ injector.py      # ContextInjector (prompt enhancement)
â”œâ”€â”€ iblm.py          # Main IBLM class (unified API)
â”œâ”€â”€ config.py        # Centralized configuration
â”œâ”€â”€ validators.py    # Input validation & sanitization
â”œâ”€â”€ security.py      # Encryption, rate limiting, integrity
â”œâ”€â”€ exceptions.py    # Custom exception hierarchy
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ demo.py            # Basic usage demo
â”‚   â””â”€â”€ production_demo.py # Production features demo
â””â”€â”€ tests/
    â””â”€â”€ test_iblm.py # Comprehensive test suite
```

---

## ğŸ“ Key Takeaways

1. **IBLM compiles interactions into rules** - not stores them
2. **Garbage collection enables infinite scaling** - source logs are deleted
3. **The user profile is a "digital twin"** - captures WHO the user is
4. **Context injection gives LLMs telepathy** - personalized from word one
5. **Corrections are high-priority learning events** - the system self-corrects
6. **Everything exports to one file** - complete portability

---

## ğŸš€ The Future

IBLM is designed to be the foundation for:
- **Multi-session memory** - remember users across conversations
- **Team kernels** - shared context for teams
- **Kernel merging** - combine knowledge from multiple sources
- **Continuous learning** - always improving understanding
- **Cross-LLM portability** - same brain, any model

The goal: **AI that truly knows you.**


---

## ğŸš€ v3.1 Enhancements (Implemented)

### Socratic Conflict Resolution
- **Problem**: Silent assumptions lead to user frustration.
- **Solution**: When a new signal contradicts an **ESTABLISHED** rule, the system *asks* the user for clarification instead of guessing.
- **Mechanism**: `CollaborationRequest` object triggers a UI interaction.

### IDE Extension Optimization
- **Terminal Noise Filtering**: Automatically ignores `ls`, `cd`, `npm install` and other transient terminal outputs.
- **Lean Context**: Ensures only meaningful developer actions are learned use `TerminalFilter`.
